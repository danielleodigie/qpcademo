<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classical PCA</title>
    <link rel="stylesheet" href="../styles/styles.css">
</head>
<body>
    <header>
        <nav >
            <div class="nav">
                <a href="/"class="homelink" class="links" ><img src="images/us.png" height="100px" id="img"/></a>
                <div class="nav_options">
                    <a class="links" href="/pages/classicalpca.html">Classical PCA</a>
                    <a class="links" href="/pages/quantumpca.html">Quantum PCA</a>
                </div>
            </div>
        </nav>
    </header>
    <div id="title">
        <h1>Classical PCA</h1>
        <h3>How to use PCA on a classical computer</h3>
    </div>
    <div class="notes">
        <div class="container">
            <h4>What is PCA?</h4>
            <p>
                Principal Component Analysis is a data analysis method that allows you to reduce the size of your data based on its principal components.
                This computation is done based on the eigenvalues (which represent the impact of a variable) and the eigenvector (which represent the principal components) of the data matrix.
            </p>
        </div>
        <div class="container">
            <h4>Why PCA?</h4>
            <p>
                <ul>
                    <li>PCA helps make multidimensional data easier to visualize by reducing the amount of components to those that have bigger impacts in the resulting data.</li>
                </ul>
            </p>
        </div>
        <div class="container">
            <h4>PCA Step by Step:</h4>
            <ul>
                <li><p>First step is to centralize the matrix, we do this by subtracting each column of the matrix by its mean. This will help us work center of our data rather than just working from the origin.</p></li>
                <li><p>Computing the covariance matrix: Next we compute the covariance matrix by variable, i.e. each column’s variance will be compared other column’s variances and itself.</p></li>
                <li><p>Find the eigenvalues and eigenvectors of our covariance matrix: using numpy, we compute the eigenvalues and vectors from our matrix. Notice that we get 1 eigenvalue per variable and each eigenvalue represents the amount of importance of said variable. The eigenvectors represent our principal component planes. We will store our vectors in a matrix.
                </p></li>
                <li><p>Organizing eigenvalues and eigenvectors. We organize the eigenvalues and vectors in descending order, based on the highest eigenvalue.</p></li>
                <li><p>Finally we project our data into the principal components by multiplying our matrix with the eigenvector matrix.</p></li>
            </ul>
        </div>
    </div>
    <div id="buttons">
        <a href="/" class="button">Home</a>
        <a href="/pages/quantumpca.html" class="button">Quantum PCA</a>
    </div>
    <footer>
        <p>"Q-ute PCA for the slay"</p>
        <ul class="contact">
            <li><a href="https://www.linkedin.com/in/yelissa-lopez-b90a0218b/ " target="_blank">Yelissa's Linkedin</a></li>
            <li><a href="https://www.linkedin.com/in/danielleodigie101/" target="_blank">Danielle's Linkedin</a></li>
            <li><a href="https://github.com/raynelfss">Raynel Sanchez</a></li>
            <li><a href="https://www.linkedin.com/in/keilydluna/" target="_blank">Keily's Linkedin</a></li>
            <li><a href="https://www.linkedin.com/in/caleb-aguirre-leon/" target="_blank">Caleb's Linkedin</a></li>
        </ul>
    </footer>
</body>
</html>